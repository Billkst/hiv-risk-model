"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
æä¾›å„ç§è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–
"""

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score
)
from sklearn.preprocessing import label_binarize


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°ç±»"""
    
    def __init__(self, model_name="Model"):
        self.model_name = model_name
        
    def evaluate(self, y_true, y_pred, y_pred_proba=None):
        """å®Œæ•´çš„æ¨¡å‹è¯„ä¼°"""
        print("\n" + "=" * 60)
        print(f"{self.model_name} - è¯„ä¼°ç»“æœ")
        print("=" * 60)
        
        # 1. åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        print(f"\nå‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        
        # 2. å„ç±»åˆ«æŒ‡æ ‡ï¼ˆä½¿ç”¨ weighted å¹³å‡ï¼‰
        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        
        print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"F1åˆ†æ•° (F1-Score): {f1:.4f}")
        
        # 3. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print("-" * 60)
        report = classification_report(y_true, y_pred, zero_division=0)
        print(report)
        
        # 4. æ··æ·†çŸ©é˜µ
        print(f"æ··æ·†çŸ©é˜µ:")
        print("-" * 60)
        cm = confusion_matrix(y_true, y_pred)
        self._print_confusion_matrix(cm, y_true)
        
        # 5. ROC-AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼‰
        if y_pred_proba is not None:
            try:
                # è·å–æ‰€æœ‰ç±»åˆ«
                classes = np.unique(y_true)
                n_classes = len(classes)
                
                if n_classes > 2:
                    # å¤šåˆ†ç±»ï¼šä½¿ç”¨ one-vs-rest
                    y_true_bin = label_binarize(y_true, classes=classes)
                    
                    # ç¡®ä¿ y_pred_proba åŒ…å«æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
                    if y_pred_proba.shape[1] == n_classes:
                        auc = roc_auc_score(y_true_bin, y_pred_proba, average='weighted', multi_class='ovr')
                        print(f"\nROC-AUC (weighted): {auc:.4f}")
                    else:
                        print(f"\nâš ï¸  æ— æ³•è®¡ç®— ROC-AUCï¼šé¢„æµ‹æ¦‚ç‡ç»´åº¦ä¸åŒ¹é…")
                else:
                    # äºŒåˆ†ç±»
                    auc = roc_auc_score(y_true, y_pred_proba[:, 1])
                    print(f"\nROC-AUC: {auc:.4f}")
            except Exception as e:
                print(f"\nâš ï¸  ROC-AUC è®¡ç®—å¤±è´¥: {e}")
        
        # è¿”å›è¯„ä¼°æŒ‡æ ‡å­—å…¸
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm
        }
        
        return metrics
    
    def _print_confusion_matrix(self, cm, y_true):
        """æ‰“å°æ ¼å¼åŒ–çš„æ··æ·†çŸ©é˜µ"""
        classes = sorted(np.unique(y_true))
        
        # æ‰“å°è¡¨å¤´
        header = "å®é™…\\é¢„æµ‹"
        print(f"{header:<12}", end="")
        for cls in classes:
            print(f"ç­‰çº§{cls:<8}", end="")
        print()
        print("-" * (12 + 10 * len(classes)))
        
        # æ‰“å°æ¯ä¸€è¡Œ
        for i, cls in enumerate(classes):
            print(f"ç­‰çº§{cls:<8}", end="")
            for j in range(len(classes)):
                if i < len(cm) and j < len(cm[i]):
                    print(f"{cm[i][j]:<10}", end="")
                else:
                    print(f"{'0':<10}", end="")
            print()
    
    def compare_models(self, results_dict):
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print("=" * 80)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for model_name, metrics in results_dict.items():
            comparison_data.append({
                'æ¨¡å‹': model_name,
                'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
                'ç²¾ç¡®ç‡': f"{metrics['precision']:.4f}",
                'å¬å›ç‡': f"{metrics['recall']:.4f}",
                'F1åˆ†æ•°': f"{metrics['f1_score']:.4f}"
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        print(df_comparison.to_string(index=False))
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(results_dict.items(), key=lambda x: x[1]['f1_score'])
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (F1={best_model[1]['f1_score']:.4f})")
        
        return df_comparison
    
    def evaluate_per_class(self, y_true, y_pred):
        """æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("å„ç±»åˆ«è¯¦ç»†è¯„ä¼°")
        print("=" * 60)
        
        classes = sorted(np.unique(y_true))
        
        for cls in classes:
            # è¯¥ç±»åˆ«çš„æ ·æœ¬æ•°
            n_samples = (y_true == cls).sum()
            
            # è¯¥ç±»åˆ«çš„é¢„æµ‹æ­£ç¡®æ•°
            correct = ((y_true == cls) & (y_pred == cls)).sum()
            
            # è¯¥ç±»åˆ«çš„å‡†ç¡®ç‡
            if n_samples > 0:
                class_acc = correct / n_samples
                print(f"\nç­‰çº§ {cls}:")
                print(f"  æ ·æœ¬æ•°: {n_samples}")
                print(f"  é¢„æµ‹æ­£ç¡®: {correct}")
                print(f"  å‡†ç¡®ç‡: {class_acc:.4f}")
            else:
                print(f"\nç­‰çº§ {cls}: æ— æ ·æœ¬")


if __name__ == '__main__':
    # æµ‹è¯•è¯„ä¼°å™¨
    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    X, y = make_classification(n_samples=200, n_features=20, n_informative=15,
                               n_classes=5, n_clusters_per_class=1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    
    # è¯„ä¼°
    evaluator = ModelEvaluator("Random Forest")
    metrics = evaluator.evaluate(y_test, y_pred, y_pred_proba)
    evaluator.evaluate_per_class(y_test, y_pred)
