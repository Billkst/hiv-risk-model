"""
æœ€ç»ˆæ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨åŸå§‹3çº§æ ‡ç­¾ + æ•°æ®å¢å¼º + 5çº§æ˜ å°„è¾“å‡º
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate, KFold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import joblib
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.evaluator import ModelEvaluator


class FinalHIVRiskModel:
    """æœ€ç»ˆ HIV é£é™©è¯„ä¼°æ¨¡å‹"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.model_name = None
        
    def load_and_prepare_data(self, csv_path):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print("\n" + "=" * 80)
        print("æ•°æ®åŠ è½½ä¸å‡†å¤‡")
        print("=" * 80)
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(csv_path)
        print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
        
        # ä½¿ç”¨åŸå§‹çš„"æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«"ä½œä¸ºç›®æ ‡
        exclude_columns = ['åŒºå¿', 'æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«', 'risk_level']
        self.feature_columns = [col for col in df.columns if col not in exclude_columns]
        
        X = df[self.feature_columns].values
        y = df['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«'].values
        
        # ä¿å­˜æ„ŸæŸ“ç‡åˆ—ç´¢å¼•ï¼ˆç”¨äºåç»­5çº§æ˜ å°„ï¼‰
        if 'æ„ŸæŸ“ç‡' in self.feature_columns:
            self.infection_rate_idx = self.feature_columns.index('æ„ŸæŸ“ç‡')
        else:
            self.infection_rate_idx = None
        
        print(f"\næ•°æ®ä¿¡æ¯:")
        print(f"  ç‰¹å¾æ•°: {len(self.feature_columns)}")
        print(f"  æ ·æœ¬æ•°: {len(X)}")
        
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ (åŸå§‹3çº§):")
        unique, counts = np.unique(y, return_counts=True)
        for cls, count in zip(unique, counts):
            print(f"  ç­‰çº§ {int(cls)}: {count} æ ·æœ¬ ({count/len(y)*100:.1f}%)")
        
        return X, y, df
    
    def augment_data(self, X, y, method='smote'):
        """æ•°æ®å¢å¼º"""
        print("\n" + "=" * 80)
        print("æ•°æ®å¢å¼º")
        print("=" * 80)
        
        print(f"ä½¿ç”¨æ–¹æ³•: {method.upper()}")
        
        # æ£€æŸ¥æœ€å°ç±»åˆ«æ ·æœ¬æ•°
        unique, counts = np.unique(y, return_counts=True)
        min_samples = counts.min()
        k_neighbors = min(5, max(1, min_samples - 1))
        
        print(f"SMOTE å‚æ•°: k_neighbors={k_neighbors}")
        
        try:
            smote = SMOTE(
                sampling_strategy='auto',
                k_neighbors=k_neighbors,
                random_state=42
            )
            
            X_resampled, y_resampled = smote.fit_resample(X, y)
            
            print(f"\nå¢å¼ºåæ•°æ®åˆ†å¸ƒ:")
            unique, counts = np.unique(y_resampled, return_counts=True)
            for cls, count in zip(unique, counts):
                print(f"  ç­‰çº§ {int(cls)}: {count} æ ·æœ¬")
            
            print(f"\nâœ“ æ•°æ®å¢å¼ºå®Œæˆ")
            print(f"  åŸå§‹: {len(X)} æ ·æœ¬")
            print(f"  å¢å¼ºå: {len(X_resampled)} æ ·æœ¬")
            print(f"  å¢åŠ : {len(X_resampled) - len(X)} æ ·æœ¬ (+{(len(X_resampled)-len(X))/len(X)*100:.1f}%)")
            
            return X_resampled, y_resampled
            
        except Exception as e:
            print(f"âš ï¸  æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            print(f"  ä½¿ç”¨åŸå§‹æ•°æ®")
            return X, y
    
    def train_with_cross_validation(self, X, y):
        """ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹è®­ç»ƒï¼ˆäº¤å‰éªŒè¯ï¼‰")
        print("=" * 80)
        
        # å®šä¹‰å€™é€‰æ¨¡å‹
        models = {
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'Logistic Regression': LogisticRegression(
                max_iter=1000,
                class_weight='balanced',
                random_state=42
            )
        }
        
        # äº¤å‰éªŒè¯è¯„ä¼°
        best_score = 0
        best_model_name = None
        results = {}
        
        for name, model in models.items():
            print(f"\n{'='*60}")
            print(f"è¯„ä¼°æ¨¡å‹: {name}")
            print(f"{'='*60}")
            
            cv = KFold(n_splits=5, shuffle=True, random_state=42)
            
            cv_results = cross_validate(
                model, X, y,
                cv=cv,
                scoring=['accuracy', 'f1_weighted'],
                return_train_score=True,
                n_jobs=-1
            )
            
            test_f1 = cv_results['test_f1_weighted'].mean()
            test_f1_std = cv_results['test_f1_weighted'].std()
            
            results[name] = {
                'f1_mean': test_f1,
                'f1_std': test_f1_std,
                'model': model
            }
            
            print(f"F1åˆ†æ•°: {test_f1:.4f} Â±{test_f1_std:.4f}")
            
            if test_f1 > best_score:
                best_score = test_f1
                best_model_name = name
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        print(f"\n{'='*60}")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"   F1åˆ†æ•°: {results[best_model_name]['f1_mean']:.4f} Â±{results[best_model_name]['f1_std']:.4f}")
        print(f"{'='*60}")
        
        self.model_name = best_model_name
        self.model = results[best_model_name]['model']
        
        # åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ä½³æ¨¡å‹
        print(f"\nåœ¨å…¨éƒ¨å¢å¼ºæ•°æ®ä¸Šè®­ç»ƒæœ€ä½³æ¨¡å‹...")
        self.model.fit(X, y)
        print(f"âœ“ è®­ç»ƒå®Œæˆ")
        
        return results
    
    def map_3_to_5_levels(self, y_pred_3level, y_proba, X_original=None):
        """
        å°†3çº§é¢„æµ‹æ˜ å°„åˆ°5çº§è¾“å‡º
        
        ç­–ç•¥ï¼šç»“åˆé¢„æµ‹æ¦‚ç‡å’Œæ„ŸæŸ“ç‡
        - ç­‰çº§1 (ä½é£é™©) â†’ æ ¹æ®ç½®ä¿¡åº¦ç»†åˆ†ä¸º 1çº§(æä½) æˆ– 2çº§(ä½)
        - ç­‰çº§2 (ä¸­é£é™©) â†’ 3çº§(ä¸­)
        - ç­‰çº§3 (é«˜é£é™©) â†’ æ ¹æ®ç½®ä¿¡åº¦å’Œæ„ŸæŸ“ç‡ç»†åˆ†ä¸º 4çº§(é«˜) æˆ– 5çº§(æé«˜)
        """
        n_samples = len(y_pred_3level)
        y_pred_5level = np.zeros(n_samples, dtype=int)
        confidence = np.zeros(n_samples)
        
        for i in range(n_samples):
            pred_class = int(y_pred_3level[i])
            prob = y_proba[i]
            max_prob = prob[pred_class - 1]  # é¢„æµ‹ç±»åˆ«çš„æ¦‚ç‡
            
            # è·å–æ„ŸæŸ“ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            infection_rate = None
            if X_original is not None and self.infection_rate_idx is not None:
                infection_rate = X_original[i, self.infection_rate_idx]
            
            # æ˜ å°„é€»è¾‘
            if pred_class == 1:  # ä½é£é™©
                if max_prob > 0.8:
                    y_pred_5level[i] = 1  # æä½é£é™©
                else:
                    y_pred_5level[i] = 2  # ä½é£é™©
                confidence[i] = max_prob
                
            elif pred_class == 2:  # ä¸­é£é™©
                y_pred_5level[i] = 3  # ä¸­é£é™©
                confidence[i] = max_prob
                
            elif pred_class == 3:  # é«˜é£é™©
                # ç»“åˆæ„ŸæŸ“ç‡å’Œç½®ä¿¡åº¦
                if infection_rate is not None:
                    if infection_rate >= 1.0 or max_prob > 0.9:
                        y_pred_5level[i] = 5  # æé«˜é£é™©
                    else:
                        y_pred_5level[i] = 4  # é«˜é£é™©
                else:
                    if max_prob > 0.8:
                        y_pred_5level[i] = 5  # æé«˜é£é™©
                    else:
                        y_pred_5level[i] = 4  # é«˜é£é™©
                confidence[i] = max_prob
        
        return y_pred_5level, confidence
    
    def predict_with_5_levels(self, X):
        """é¢„æµ‹å¹¶è¾“å‡º5çº§ç»“æœ"""
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(X)
        
        # 3çº§é¢„æµ‹
        y_pred_3 = self.model.predict(X_scaled)
        y_proba = self.model.predict_proba(X_scaled)
        
        # æ˜ å°„åˆ°5çº§
        y_pred_5, confidence = self.map_3_to_5_levels(y_pred_3, y_proba, X)
        
        return y_pred_5, confidence, y_pred_3, y_proba
    
    def save_model(self, save_dir='saved_models'):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        
        model_info = {
            'model': self.model,
            'scaler': self.scaler,
            'model_name': self.model_name,
            'feature_columns': self.feature_columns,
            'infection_rate_idx': self.infection_rate_idx
        }
        
        model_path = os.path.join(save_dir, 'final_model_3to5.pkl')
        joblib.dump(model_info, model_path)
        
        print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        return model_path


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("\n" + "=" * 80)
    print("æœ€ç»ˆ HIV é£é™©è¯„ä¼°æ¨¡å‹è®­ç»ƒ")
    print("ä½¿ç”¨åŸå§‹3çº§æ ‡ç­¾ + æ•°æ®å¢å¼º + 5çº§æ˜ å°„è¾“å‡º")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = FinalHIVRiskModel()
    
    # æ­¥éª¤1: åŠ è½½æ•°æ®
    print("\nã€æ­¥éª¤ 1/6ã€‘åŠ è½½æ•°æ®")
    X, y, df = model.load_and_prepare_data('data/processed/hiv_data_processed.csv')
    
    # æ­¥éª¤2: ç‰¹å¾æ ‡å‡†åŒ–
    print("\nã€æ­¥éª¤ 2/6ã€‘ç‰¹å¾æ ‡å‡†åŒ–")
    X_scaled = model.scaler.fit_transform(X)
    print("âœ“ æ ‡å‡†åŒ–å®Œæˆ")
    
    # æ­¥éª¤3: æ•°æ®å¢å¼º
    print("\nã€æ­¥éª¤ 3/6ã€‘æ•°æ®å¢å¼º")
    X_augmented, y_augmented = model.augment_data(X_scaled, y)
    
    # æ­¥éª¤4: è®­ç»ƒæ¨¡å‹
    print("\nã€æ­¥éª¤ 4/6ã€‘æ¨¡å‹è®­ç»ƒ")
    results = model.train_with_cross_validation(X_augmented, y_augmented)
    
    # æ­¥éª¤5: æµ‹è¯•5çº§æ˜ å°„
    print("\nã€æ­¥éª¤ 5/6ã€‘æµ‹è¯•5çº§æ˜ å°„")
    print("=" * 80)
    
    # åœ¨åŸå§‹æ•°æ®ä¸Šæµ‹è¯•
    y_pred_5, confidence, y_pred_3, y_proba = model.predict_with_5_levels(X)
    
    print(f"\n5çº§é¢„æµ‹ç»“æœåˆ†å¸ƒ:")
    unique, counts = np.unique(y_pred_5, return_counts=True)
    for cls, count in zip(unique, counts):
        print(f"  ç­‰çº§ {cls}: {count} æ ·æœ¬ ({count/len(y_pred_5)*100:.1f}%)")
    
    print(f"\nå¹³å‡ç½®ä¿¡åº¦: {confidence.mean():.4f}")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\né¢„æµ‹ç¤ºä¾‹ (å‰10ä¸ªæ ·æœ¬):")
    print(f"{'3çº§é¢„æµ‹':<10} {'5çº§é¢„æµ‹':<10} {'ç½®ä¿¡åº¦':<10} {'æ¦‚ç‡åˆ†å¸ƒ'}")
    print("-" * 60)
    for i in range(min(10, len(y_pred_3))):
        probs = ', '.join([f'{p:.3f}' for p in y_proba[i]])
        print(f"{int(y_pred_3[i]):<10} {y_pred_5[i]:<10} {confidence[i]:.4f}    [{probs}]")
    
    # æ­¥éª¤6: ä¿å­˜æ¨¡å‹
    print("\nã€æ­¥éª¤ 6/6ã€‘ä¿å­˜æ¨¡å‹")
    model_path = model.save_model()
    
    print("\n" + "=" * 80)
    print("âœ“ è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("=" * 80)
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  æ¨¡å‹ç±»å‹: {model.model_name}")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(X_augmented)}")
    print(f"  ç‰¹å¾æ•°: {len(model.feature_columns)}")
    print(f"  è¾“å…¥: 3çº§æ ‡ç­¾")
    print(f"  è¾“å‡º: 5çº§é£é™©è¯„ä¼° + ç½®ä¿¡åº¦")
    
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {model_path}")
    
    return model, results


if __name__ == '__main__':
    model, results = main()
