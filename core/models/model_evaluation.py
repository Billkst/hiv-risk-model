"""
æ¨¡å‹æ·±åº¦è¯„ä¼°
è¯„ä¼°æ¨¡å‹çš„å¯é æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®éœ€æ±‚
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import learning_curve, validation_curve, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import joblib
import warnings
warnings.filterwarnings('ignore')


class ModelEvaluator:
    """æ¨¡å‹æ·±åº¦è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results = {}
        
    def load_data(self, csv_path):
        """åŠ è½½æ•°æ®"""
        print("\n" + "=" * 80)
        print("åŠ è½½æ•°æ®")
        print("=" * 80)
        
        df = pd.read_csv(csv_path)
        
        # ä½¿ç”¨åŸå§‹3çº§æ ‡ç­¾
        exclude_columns = ['åŒºå¿', 'æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«', 'risk_level']
        feature_columns = [col for col in df.columns if col not in exclude_columns]
        
        X = df[feature_columns].values
        y = df['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«'].values
        
        print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  æ ·æœ¬æ•°: {len(X)}")
        print(f"  ç‰¹å¾æ•°: {len(feature_columns)}")
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        return X_scaled, y, feature_columns
    
    def evaluate_sample_size_impact(self, X, y):
        """è¯„ä¼°æ ·æœ¬é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼ˆå­¦ä¹ æ›²çº¿ï¼‰"""
        print("\n" + "=" * 80)
        print("è¯„ä¼°1: å­¦ä¹ æ›²çº¿åˆ†æ")
        print("=" * 80)
        print("ç›®çš„: åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®")
        
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        # è®¡ç®—å­¦ä¹ æ›²çº¿
        train_sizes = np.linspace(0.1, 1.0, 10)
        
        print("\nè®¡ç®—å­¦ä¹ æ›²çº¿...")
        train_sizes_abs, train_scores, val_scores = learning_curve(
            model, X, y,
            train_sizes=train_sizes,
            cv=5,
            scoring='f1_weighted',
            n_jobs=-1,
            random_state=42
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)
        
        # åˆ†æç»“æœ
        print("\nå­¦ä¹ æ›²çº¿ç»“æœ:")
        print(f"{'æ ·æœ¬æ•°':<10} {'è®­ç»ƒF1':<15} {'éªŒè¯F1':<15} {'å·®è·':<10}")
        print("-" * 50)
        
        for i, size in enumerate(train_sizes_abs):
            gap = train_mean[i] - val_mean[i]
            print(f"{int(size):<10} {train_mean[i]:.4f}Â±{train_std[i]:.4f}  {val_mean[i]:.4f}Â±{val_std[i]:.4f}  {gap:.4f}")
        
        # åˆ¤æ–­
        final_gap = train_mean[-1] - val_mean[-1]
        final_val_score = val_mean[-1]
        
        print(f"\nåˆ†æç»“è®º:")
        print(f"  æœ€ç»ˆéªŒè¯F1: {final_val_score:.4f}")
        print(f"  è®­ç»ƒ-éªŒè¯å·®è·: {final_gap:.4f}")
        
        if final_gap > 0.1:
            print(f"  âš ï¸  å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œä½†å¯èƒ½æ˜¯æ•°æ®é‡ä¸è¶³å¯¼è‡´")
        else:
            print(f"  âœ“ æ¨¡å‹æ³›åŒ–è‰¯å¥½")
        
        if val_mean[-1] < val_mean[-2]:
            print(f"  âš ï¸  éªŒè¯æ€§èƒ½æœªéšæ ·æœ¬å¢åŠ è€Œæå‡ï¼Œå¯èƒ½å·²è¾¾åˆ°æ•°æ®ä¸Šé™")
        else:
            print(f"  âœ“ éªŒè¯æ€§èƒ½éšæ ·æœ¬å¢åŠ è€Œæå‡ï¼Œæ›´å¤šæ•°æ®å¯èƒ½æœ‰å¸®åŠ©")
        
        # ä¿å­˜ç»“æœ
        self.results['learning_curve'] = {
            'train_sizes': train_sizes_abs,
            'train_scores': train_mean,
            'val_scores': val_mean,
            'final_gap': final_gap,
            'final_val_score': final_val_score
        }
        
        return train_sizes_abs, train_mean, val_mean
    
    def evaluate_data_augmentation_impact(self, X, y):
        """è¯„ä¼°æ•°æ®å¢å¼ºçš„å½±å“"""
        print("\n" + "=" * 80)
        print("è¯„ä¼°2: æ•°æ®å¢å¼ºæ•ˆæœ")
        print("=" * 80)
        print("ç›®çš„: éªŒè¯SMOTEæ˜¯å¦çœŸæ­£æå‡æ€§èƒ½")
        
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        # 1. åŸå§‹æ•°æ®æ€§èƒ½
        print("\næµ‹è¯•1: åŸå§‹æ•°æ®ï¼ˆæ— å¢å¼ºï¼‰")
        scores_original = cross_val_score(
            model, X, y,
            cv=5,
            scoring='f1_weighted',
            n_jobs=-1
        )
        
        print(f"  F1åˆ†æ•°: {scores_original.mean():.4f} Â±{scores_original.std():.4f}")
        
        # 2. SMOTEå¢å¼ºåæ€§èƒ½
        print("\næµ‹è¯•2: SMOTEå¢å¼ºæ•°æ®")
        
        # æ£€æŸ¥æœ€å°ç±»åˆ«æ ·æœ¬æ•°
        unique, counts = np.unique(y, return_counts=True)
        min_samples = counts.min()
        k_neighbors = min(5, max(1, min_samples - 1))
        
        smote = SMOTE(k_neighbors=k_neighbors, random_state=42)
        X_smote, y_smote = smote.fit_resample(X, y)
        
        print(f"  å¢å¼ºåæ ·æœ¬æ•°: {len(X_smote)} (åŸå§‹: {len(X)})")
        
        scores_smote = cross_val_score(
            model, X_smote, y_smote,
            cv=5,
            scoring='f1_weighted',
            n_jobs=-1
        )
        
        print(f"  F1åˆ†æ•°: {scores_smote.mean():.4f} Â±{scores_smote.std():.4f}")
        
        # 3. å¯¹æ¯”åˆ†æ
        improvement = scores_smote.mean() - scores_original.mean()
        
        print(f"\nå¯¹æ¯”åˆ†æ:")
        print(f"  æ€§èƒ½æå‡: {improvement:+.4f}")
        
        if improvement > 0.02:
            print(f"  âœ“ SMOTEæ˜¾è‘—æå‡æ€§èƒ½ï¼Œæ•°æ®å¢å¼ºæœ‰æ•ˆ")
        elif improvement > 0:
            print(f"  â‰ˆ SMOTEç•¥å¾®æå‡æ€§èƒ½")
        else:
            print(f"  âš ï¸  SMOTEæœªæå‡æ€§èƒ½ï¼Œå¯èƒ½å¼•å…¥å™ªå£°")
        
        self.results['augmentation'] = {
            'original_score': scores_original.mean(),
            'smote_score': scores_smote.mean(),
            'improvement': improvement
        }
        
        return scores_original, scores_smote
    
    def evaluate_model_stability(self, X, y):
        """è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§ï¼ˆå¤šæ¬¡éšæœºåˆ’åˆ†ï¼‰"""
        print("\n" + "=" * 80)
        print("è¯„ä¼°3: æ¨¡å‹ç¨³å®šæ€§")
        print("=" * 80)
        print("ç›®çš„: æ£€æŸ¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®åˆ’åˆ†ä¸‹çš„è¡¨ç°")
        
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        # ä½¿ç”¨ä¸åŒéšæœºç§å­è¿›è¡Œå¤šæ¬¡äº¤å‰éªŒè¯
        print("\nè¿›è¡Œ20æ¬¡äº¤å‰éªŒè¯ï¼ˆä¸åŒéšæœºç§å­ï¼‰...")
        
        all_scores = []
        for seed in range(20):
            scores = cross_val_score(
                model, X, y,
                cv=5,
                scoring='f1_weighted',
                n_jobs=-1
            )
            all_scores.append(scores.mean())
        
        all_scores = np.array(all_scores)
        
        print(f"\nç¨³å®šæ€§åˆ†æ:")
        print(f"  å¹³å‡F1: {all_scores.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {all_scores.std():.4f}")
        print(f"  æœ€å°å€¼: {all_scores.min():.4f}")
        print(f"  æœ€å¤§å€¼: {all_scores.max():.4f}")
        print(f"  å˜å¼‚ç³»æ•°: {all_scores.std()/all_scores.mean()*100:.2f}%")
        
        if all_scores.std() < 0.05:
            print(f"  âœ“ æ¨¡å‹éå¸¸ç¨³å®š")
        elif all_scores.std() < 0.1:
            print(f"  âœ“ æ¨¡å‹è¾ƒç¨³å®š")
        else:
            print(f"  âš ï¸  æ¨¡å‹ä¸ç¨³å®šï¼Œæ€§èƒ½æ³¢åŠ¨è¾ƒå¤§")
        
        self.results['stability'] = {
            'mean': all_scores.mean(),
            'std': all_scores.std(),
            'min': all_scores.min(),
            'max': all_scores.max()
        }
        
        return all_scores
    
    def evaluate_feature_importance_stability(self, X, y, feature_columns):
        """è¯„ä¼°ç‰¹å¾é‡è¦æ€§çš„ç¨³å®šæ€§"""
        print("\n" + "=" * 80)
        print("è¯„ä¼°4: ç‰¹å¾é‡è¦æ€§ç¨³å®šæ€§")
        print("=" * 80)
        print("ç›®çš„: æ£€æŸ¥å…³é”®ç‰¹å¾æ˜¯å¦ä¸€è‡´")
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œè®°å½•ç‰¹å¾é‡è¦æ€§
        n_iterations = 10
        importance_matrix = []
        
        print(f"\nè®­ç»ƒ{n_iterations}ä¸ªæ¨¡å‹...")
        
        for i in range(n_iterations):
            model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=i
            )
            model.fit(X, y)
            importance_matrix.append(model.feature_importances_)
        
        importance_matrix = np.array(importance_matrix)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡é‡è¦æ€§å’Œæ ‡å‡†å·®
        mean_importance = importance_matrix.mean(axis=0)
        std_importance = importance_matrix.std(axis=0)
        
        # æ‰¾å‡ºTop 10ç‰¹å¾
        top_indices = np.argsort(mean_importance)[-10:][::-1]
        
        print(f"\nTop 10 é‡è¦ç‰¹å¾:")
        print(f"{'ç‰¹å¾':<40} {'å¹³å‡é‡è¦æ€§':<15} {'æ ‡å‡†å·®':<15}")
        print("-" * 70)
        
        for idx in top_indices:
            feat_name = feature_columns[idx]
            mean_imp = mean_importance[idx]
            std_imp = std_importance[idx]
            cv = std_imp / mean_imp if mean_imp > 0 else 0
            
            print(f"{feat_name:<40} {mean_imp:.6f}       {std_imp:.6f} (CV:{cv:.2%})")
        
        # åˆ†æ
        top_feature = feature_columns[top_indices[0]]
        top_importance = mean_importance[top_indices[0]]
        
        print(f"\nåˆ†æ:")
        print(f"  æœ€é‡è¦ç‰¹å¾: {top_feature} ({top_importance:.4f})")
        
        if top_importance > 0.5:
            print(f"  âš ï¸  å•ä¸ªç‰¹å¾ä¸»å¯¼æ€§è¿‡å¼ºï¼ˆ>{top_importance:.1%}ï¼‰ï¼Œæ¨¡å‹å¯èƒ½è¿‡åº¦ä¾èµ–è¯¥ç‰¹å¾")
        else:
            print(f"  âœ“ ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒè¾ƒå‡è¡¡")
        
        self.results['feature_importance'] = {
            'top_feature': top_feature,
            'top_importance': top_importance,
            'mean_importance': mean_importance,
            'std_importance': std_importance
        }
        
        return mean_importance, std_importance, feature_columns
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)
        
        print("\nğŸ“Š æ•°æ®é‡è¯„ä¼°:")
        if 'learning_curve' in self.results:
            lc = self.results['learning_curve']
            print(f"  å½“å‰éªŒè¯F1: {lc['final_val_score']:.4f}")
            print(f"  è®­ç»ƒ-éªŒè¯å·®è·: {lc['final_gap']:.4f}")
            
            if lc['final_gap'] > 0.15:
                print(f"  âŒ æ•°æ®é‡ä¸¥é‡ä¸è¶³ï¼Œå¼ºçƒˆå»ºè®®å¢åŠ æ•°æ®")
                data_recommendation = "éœ€è¦æ›´å¤šçœŸå®æ•°æ®æˆ–é«˜è´¨é‡åˆæˆæ•°æ®"
            elif lc['final_gap'] > 0.1:
                print(f"  âš ï¸  æ•°æ®é‡å¯èƒ½ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ•°æ®")
                data_recommendation = "å»ºè®®å¢åŠ æ•°æ®ä»¥æå‡æ³›åŒ–èƒ½åŠ›"
            else:
                print(f"  âœ“ æ•°æ®é‡åŸºæœ¬å……è¶³")
                data_recommendation = "å½“å‰æ•°æ®é‡å¯æ¥å—"
        
        print("\nğŸ”„ æ•°æ®å¢å¼ºè¯„ä¼°:")
        if 'augmentation' in self.results:
            aug = self.results['augmentation']
            print(f"  åŸå§‹æ•°æ®F1: {aug['original_score']:.4f}")
            print(f"  SMOTEå¢å¼ºF1: {aug['smote_score']:.4f}")
            print(f"  æ€§èƒ½æå‡: {aug['improvement']:+.4f}")
            
            if aug['improvement'] > 0.02:
                print(f"  âœ“ SMOTEæœ‰æ•ˆï¼Œå»ºè®®ä½¿ç”¨")
                augmentation_recommendation = "ä½¿ç”¨SMOTEæ•°æ®å¢å¼º"
            else:
                print(f"  âš ï¸  SMOTEæ•ˆæœæœ‰é™")
                augmentation_recommendation = "è€ƒè™‘å…¶ä»–æ•°æ®å¢å¼ºæ–¹æ³•"
        
        print("\nğŸ“ˆ æ¨¡å‹ç¨³å®šæ€§:")
        if 'stability' in self.results:
            stab = self.results['stability']
            print(f"  å¹³å‡F1: {stab['mean']:.4f}")
            print(f"  æ ‡å‡†å·®: {stab['std']:.4f}")
            print(f"  èŒƒå›´: [{stab['min']:.4f}, {stab['max']:.4f}]")
            
            if stab['std'] < 0.05:
                print(f"  âœ“ æ¨¡å‹ç¨³å®šæ€§ä¼˜ç§€")
                stability_recommendation = "æ¨¡å‹å¯é "
            elif stab['std'] < 0.1:
                print(f"  âœ“ æ¨¡å‹ç¨³å®šæ€§è‰¯å¥½")
                stability_recommendation = "æ¨¡å‹åŸºæœ¬å¯é "
            else:
                print(f"  âš ï¸  æ¨¡å‹ä¸ç¨³å®š")
                stability_recommendation = "éœ€è¦æ›´å¤šæ•°æ®æˆ–è°ƒæ•´æ¨¡å‹"
        
        print("\nğŸ¯ ç‰¹å¾ä¾èµ–æ€§:")
        if 'feature_importance' in self.results:
            fi = self.results['feature_importance']
            print(f"  æœ€é‡è¦ç‰¹å¾: {fi['top_feature']}")
            print(f"  é‡è¦æ€§: {fi['top_importance']:.4f}")
            
            if fi['top_importance'] > 0.5:
                print(f"  âš ï¸  è¿‡åº¦ä¾èµ–å•ä¸€ç‰¹å¾")
                feature_recommendation = "è€ƒè™‘æ’é™¤ä¸»å¯¼ç‰¹å¾é‡æ–°è®­ç»ƒ"
            else:
                print(f"  âœ“ ç‰¹å¾ä½¿ç”¨å‡è¡¡")
                feature_recommendation = "ç‰¹å¾å·¥ç¨‹åˆç†"
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æœ€ç»ˆå»ºè®®")
        print("=" * 80)
        
        print(f"\n1. æ•°æ®æ–¹é¢: {data_recommendation}")
        print(f"2. æ•°æ®å¢å¼º: {augmentation_recommendation}")
        print(f"3. æ¨¡å‹ç¨³å®šæ€§: {stability_recommendation}")
        print(f"4. ç‰¹å¾å·¥ç¨‹: {feature_recommendation}")
        
        # æ€»ä½“å»ºè®®
        print(f"\nğŸ’¡ æ€»ä½“å»ºè®®:")
        
        if 'learning_curve' in self.results and self.results['learning_curve']['final_gap'] > 0.15:
            print(f"  âŒ å½“å‰æ¨¡å‹ä¸é€‚åˆç”Ÿäº§ä½¿ç”¨")
            print(f"     - æ•°æ®é‡ä¸¥é‡ä¸è¶³ï¼ˆä»…190æ ·æœ¬ï¼‰")
            print(f"     - å»ºè®®æ”¶é›†æ›´å¤šçœŸå®æ•°æ®")
            print(f"     - æˆ–ä½¿ç”¨é«˜çº§åˆæˆæ•°æ®æ–¹æ³•ï¼ˆå¦‚VAEã€GANï¼‰")
        elif 'learning_curve' in self.results and self.results['learning_curve']['final_gap'] > 0.1:
            print(f"  âš ï¸  æ¨¡å‹å¯ç”¨äºç ”ç©¶ï¼Œä½†ç”Ÿäº§ä½¿ç”¨éœ€è°¨æ…")
            print(f"     - å»ºè®®å¢åŠ æ•°æ®é‡åˆ°500+æ ·æœ¬")
            print(f"     - ç»§ç»­ä½¿ç”¨SMOTEç­‰æ•°æ®å¢å¼º")
        else:
            print(f"  âœ“ æ¨¡å‹åŸºæœ¬å¯ç”¨")
            print(f"     - æ€§èƒ½å’Œç¨³å®šæ€§å¯æ¥å—")
            print(f"     - å»ºè®®æŒç»­æ”¶é›†æ•°æ®æ”¹è¿›")


def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    print("\n" + "=" * 80)
    print("HIVé£é™©æ¨¡å‹æ·±åº¦è¯„ä¼°")
    print("=" * 80)
    
    evaluator = ModelEvaluator()
    
    # åŠ è½½æ•°æ®
    X, y, feature_columns = evaluator.load_data('data/processed/hiv_data_processed.csv')
    
    # è¯„ä¼°1: å­¦ä¹ æ›²çº¿
    evaluator.evaluate_sample_size_impact(X, y)
    
    # è¯„ä¼°2: æ•°æ®å¢å¼ºæ•ˆæœ
    evaluator.evaluate_data_augmentation_impact(X, y)
    
    # è¯„ä¼°3: æ¨¡å‹ç¨³å®šæ€§
    evaluator.evaluate_model_stability(X, y)
    
    # è¯„ä¼°4: ç‰¹å¾é‡è¦æ€§
    evaluator.evaluate_feature_importance_stability(X, y, feature_columns)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    evaluator.generate_comprehensive_report()
    
    print("\n" + "=" * 80)
    print("âœ“ è¯„ä¼°å®Œæˆ")
    print("=" * 80)
    
    return evaluator


if __name__ == '__main__':
    evaluator = main()
