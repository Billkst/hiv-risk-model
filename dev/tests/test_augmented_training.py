"""
æµ‹è¯•ä¸åŒæ•°é‡çš„åˆæˆæ•°æ®å¢å¼ºæ•ˆæœ
ç­–ç•¥: 190çœŸå®æ•°æ® + Næ¡åˆæˆæ•°æ®
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sdv.single_table import CTGANSynthesizer
from sdv.metadata import SingleTableMetadata
import warnings
warnings.filterwarnings('ignore')


def generate_more_synthetic_data(real_df, n_synthetic, model_path=None):
    """ç”Ÿæˆæ›´å¤šåˆæˆæ•°æ®"""
    print(f"\nç”Ÿæˆ {n_synthetic} æ¡åˆæˆæ•°æ®...")
    
    if model_path:
        # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        try:
            synthesizer = CTGANSynthesizer.load(model_path)
            print(f"âœ“ åŠ è½½å·²æœ‰CTGANæ¨¡å‹")
        except:
            print(f"âš ï¸  æ— æ³•åŠ è½½æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒ...")
            model_path = None
    
    if not model_path:
        # é‡æ–°è®­ç»ƒ
        print(f"è®­ç»ƒCTGANæ¨¡å‹...")
        metadata = SingleTableMetadata()
        metadata.detect_from_dataframe(real_df)
        
        synthesizer = CTGANSynthesizer(
            metadata=metadata,
            epochs=300,
            batch_size=100,
            verbose=False
        )
        synthesizer.fit(real_df)
        print(f"âœ“ è®­ç»ƒå®Œæˆ")
    
    # ç”Ÿæˆæ•°æ®
    synthetic_df = synthesizer.sample(num_rows=n_synthetic)
    print(f"âœ“ ç”Ÿæˆå®Œæˆ: {synthetic_df.shape}")
    
    return synthetic_df


def create_augmented_dataset(real_df, n_synthetic):
    """åˆ›å»ºå¢å¼ºæ•°æ®é›†ï¼šå…¨éƒ¨çœŸå®æ•°æ® + Næ¡åˆæˆæ•°æ®"""
    print(f"\n" + "=" * 80)
    print(f"åˆ›å»ºå¢å¼ºæ•°æ®é›†: 190çœŸå® + {n_synthetic}åˆæˆ")
    print("=" * 80)
    
    # ç”Ÿæˆåˆæˆæ•°æ®
    synthetic_df = generate_more_synthetic_data(
        real_df, 
        n_synthetic,
        model_path='saved_models/ctgan_model.pkl'
    )
    
    # åˆå¹¶ï¼šå…¨éƒ¨çœŸå®æ•°æ® + åˆæˆæ•°æ®
    augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)
    
    # æ‰“ä¹±é¡ºåº
    augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"\nå¢å¼ºæ•°æ®é›†:")
    print(f"  çœŸå®æ•°æ®: {len(real_df)} æ ·æœ¬ (100%çœŸå®æ•°æ®)")
    print(f"  åˆæˆæ•°æ®: {n_synthetic} æ ·æœ¬")
    print(f"  æ€»è®¡: {len(augmented_df)} æ ·æœ¬")
    
    # æ˜¾ç¤ºç›®æ ‡å˜é‡åˆ†å¸ƒ
    if 'æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«' in augmented_df.columns:
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        for level in sorted(augmented_df['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«'].unique()):
            count = (augmented_df['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«'] == level).sum()
            pct = count / len(augmented_df) * 100
            print(f"  ç­‰çº§ {int(level)}: {count} æ ·æœ¬ ({pct:.1f}%)")
    
    return augmented_df


def evaluate_augmented_dataset(augmented_df, dataset_name):
    """è¯„ä¼°å¢å¼ºæ•°æ®é›†"""
    print(f"\nè¯„ä¼°: {dataset_name}")
    
    # å‡†å¤‡æ•°æ®
    X = augmented_df.drop(columns=['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«']).values
    y = augmented_df['æŒ‰æ–¹æ¡ˆè¯„å®šçº§åˆ«'].values
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # æ¨¡å‹
    model = GradientBoostingClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )
    
    # äº¤å‰éªŒè¯
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    scores = cross_val_score(
        model, X_scaled, y,
        cv=cv,
        scoring='f1_weighted',
        n_jobs=-1
    )
    
    mean_score = scores.mean()
    std_score = scores.std()
    
    print(f"  F1åˆ†æ•°: {mean_score:.4f} Â±{std_score:.4f}")
    
    return {
        'dataset': dataset_name,
        'n_samples': len(X),
        'f1_mean': mean_score,
        'f1_std': std_score
    }


def test_different_augmentation_levels():
    """æµ‹è¯•ä¸åŒæ•°é‡çš„åˆæˆæ•°æ®å¢å¼º"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•ä¸åŒæ•°é‡çš„åˆæˆæ•°æ®å¢å¼º")
    print("=" * 80)
    
    # åŠ è½½çœŸå®æ•°æ®
    print("\nåŠ è½½çœŸå®æ•°æ®...")
    df = pd.read_csv('data/processed/hiv_data_processed.csv')
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    exclude_columns = ['åŒºå¿', 'risk_level']
    real_df = df.drop(columns=exclude_columns)
    
    print(f"âœ“ çœŸå®æ•°æ®: {real_df.shape}")
    
    # æµ‹è¯•ä¸åŒçš„å¢å¼ºçº§åˆ«
    augmentation_levels = [
        0,      # åŸºçº¿ï¼šä»…çœŸå®æ•°æ®
        200,    # 190 + 200 = 390
        500,    # 190 + 500 = 690
        1000,   # 190 + 1000 = 1190
        2000,   # 190 + 2000 = 2190
    ]
    
    results = []
    
    for n_synthetic in augmentation_levels:
        if n_synthetic == 0:
            # åŸºçº¿ï¼šä»…çœŸå®æ•°æ®
            dataset_name = f"ä»…çœŸå®æ•°æ®({len(real_df)}æ ·æœ¬)"
            result = evaluate_augmented_dataset(real_df, dataset_name)
        else:
            # å¢å¼ºæ•°æ®
            augmented_df = create_augmented_dataset(real_df, n_synthetic)
            dataset_name = f"çœŸå®+åˆæˆ({len(real_df)}+{n_synthetic}={len(augmented_df)}æ ·æœ¬)"
            result = evaluate_augmented_dataset(augmented_df, dataset_name)
        
        results.append(result)
    
    # æ±‡æ€»å¯¹æ¯”
    print("\n" + "=" * 80)
    print("æ€§èƒ½æ±‡æ€»å¯¹æ¯”")
    print("=" * 80)
    
    print(f"\n{'æ•°æ®é›†é…ç½®':<50} {'æ€»æ ·æœ¬æ•°':<12} {'F1åˆ†æ•°':<20}")
    print("-" * 82)
    
    baseline_f1 = results[0]['f1_mean']
    
    for result in results:
        dataset = result['dataset']
        n_samples = result['n_samples']
        f1_str = f"{result['f1_mean']:.4f} Â±{result['f1_std']:.4f}"
        
        # è®¡ç®—ç›¸å¯¹åŸºçº¿çš„æå‡
        improvement = result['f1_mean'] - baseline_f1
        if improvement != 0:
            improvement_str = f"({improvement:+.4f})"
        else:
            improvement_str = "(åŸºçº¿)"
        
        print(f"{dataset:<50} {n_samples:<12} {f1_str:<20} {improvement_str}")
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_result = max(results, key=lambda x: x['f1_mean'])
    
    print("\n" + "=" * 80)
    print("åˆ†æç»“è®º")
    print("=" * 80)
    
    print(f"\nğŸ† æœ€ä½³é…ç½®: {best_result['dataset']}")
    print(f"   F1åˆ†æ•°: {best_result['f1_mean']:.4f} Â±{best_result['f1_std']:.4f}")
    
    improvement = best_result['f1_mean'] - baseline_f1
    improvement_pct = improvement / baseline_f1 * 100
    
    print(f"\nğŸ“Š ç›¸æ¯”åŸºçº¿(ä»…çœŸå®æ•°æ®):")
    print(f"   åŸºçº¿F1: {baseline_f1:.4f}")
    print(f"   æœ€ä½³F1: {best_result['f1_mean']:.4f}")
    print(f"   æå‡: {improvement:+.4f} ({improvement_pct:+.2f}%)")
    
    if improvement > 0.05:
        print(f"\nâœ“ åˆæˆæ•°æ®æ˜¾è‘—æå‡æ€§èƒ½ï¼")
        print(f"  å»ºè®®: ä½¿ç”¨ {best_result['dataset']} é…ç½®")
    elif improvement > 0.02:
        print(f"\nâœ“ åˆæˆæ•°æ®æœ‰æ•ˆæå‡æ€§èƒ½")
        print(f"  å»ºè®®: ä½¿ç”¨ {best_result['dataset']} é…ç½®")
    elif improvement > 0:
        print(f"\nâ‰ˆ åˆæˆæ•°æ®ç•¥å¾®æå‡æ€§èƒ½")
        print(f"  å»ºè®®: å¯ä»¥ä½¿ç”¨ {best_result['dataset']} é…ç½®")
    else:
        print(f"\nâš ï¸  åˆæˆæ•°æ®æœªæå‡æ€§èƒ½")
        print(f"  å»ºè®®: ç»§ç»­ä½¿ç”¨çœŸå®æ•°æ®")
    
    # ä¿å­˜æœ€ä½³é…ç½®çš„æ•°æ®
    if improvement > 0:
        print(f"\nä¿å­˜æœ€ä½³é…ç½®æ•°æ®...")
        
        # æ‰¾å‡ºæœ€ä½³çš„åˆæˆæ•°æ®æ•°é‡
        best_n_synthetic = None
        for i, result in enumerate(results):
            if result == best_result and i > 0:
                best_n_synthetic = augmentation_levels[i]
                break
        
        if best_n_synthetic:
            best_augmented_df = create_augmented_dataset(real_df, best_n_synthetic)
            output_path = f'data/processed/hiv_best_augmented_{len(real_df)}+{best_n_synthetic}.csv'
            best_augmented_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"âœ“ æœ€ä½³é…ç½®æ•°æ®å·²ä¿å­˜: {output_path}")
    
    return results


def main():
    """ä¸»æµç¨‹"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•åˆæˆæ•°æ®å¢å¼ºç­–ç•¥")
    print("ç­–ç•¥: ä¿ç•™å…¨éƒ¨190æ¡çœŸå®æ•°æ® + å¢åŠ Næ¡åˆæˆæ•°æ®")
    print("=" * 80)
    
    results = test_different_augmentation_levels()
    
    print("\n" + "=" * 80)
    print("âœ“ æµ‹è¯•å®Œæˆ")
    print("=" * 80)
    
    return results


if __name__ == '__main__':
    results = main()
