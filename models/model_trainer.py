"""
æ¨¡å‹è®­ç»ƒä¸»è„šæœ¬
è®­ç»ƒå¤šä¸ªåŸºçº¿æ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import joblib
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.feature_engineer import FeatureEngineer
from models.evaluator import ModelEvaluator


class HIVRiskModelTrainer:
    """HIV é£é™©æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.models = {}
        self.results = {}
        self.best_model = None
        self.best_model_name = None
        
    def initialize_models(self):
        """åˆå§‹åŒ–å¤šä¸ªåŸºçº¿æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("åˆå§‹åŒ–æ¨¡å‹")
        print("=" * 80)
        
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡é—®é¢˜ï¼‰
        self.models = {
            'Logistic Regression': LogisticRegression(
                max_iter=1000,
                class_weight='balanced',  # è‡ªåŠ¨å¹³è¡¡ç±»åˆ«æƒé‡
                random_state=42
            ),
            
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            ),
            
            'SVM': SVC(
                kernel='rbf',
                class_weight='balanced',
                probability=True,  # å¯ç”¨æ¦‚ç‡é¢„æµ‹
                random_state=42
            )
        }
        
        print(f"âœ“ åˆå§‹åŒ–äº† {len(self.models)} ä¸ªæ¨¡å‹:")
        for i, name in enumerate(self.models.keys(), 1):
            print(f"  {i}. {name}")
    
    def train_model(self, model_name, model, X_train, y_train, X_val, y_val):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print("\n" + "=" * 80)
        print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        print("=" * 80)
        
        # è®­ç»ƒ
        print(f"å¼€å§‹è®­ç»ƒ...")
        model.fit(X_train, y_train)
        print(f"âœ“ è®­ç»ƒå®Œæˆ")
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        print(f"\nåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°:")
        y_val_pred = model.predict(X_val)
        
        # è·å–æ¦‚ç‡é¢„æµ‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
        try:
            y_val_pred_proba = model.predict_proba(X_val)
        except:
            y_val_pred_proba = None
        
        # è¯„ä¼°
        evaluator = ModelEvaluator(model_name)
        metrics = evaluator.evaluate(y_val, y_val_pred, y_val_pred_proba)
        evaluator.evaluate_per_class(y_val, y_val_pred)
        
        return model, metrics
    
    def train_all_models(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
        print("=" * 80)
        
        for model_name, model in self.models.items():
            trained_model, metrics = self.train_model(
                model_name, model, X_train, y_train, X_val, y_val
            )
            
            # ä¿å­˜ç»“æœ
            self.results[model_name] = {
                'model': trained_model,
                'metrics': metrics
            }
        
        print("\n" + "=" * 80)
        print("âœ“ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print("=" * 80)
    
    def compare_models(self):
        """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
        print("\n" + "=" * 80)
        print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print("=" * 80)
        
        # å‡†å¤‡å¯¹æ¯”æ•°æ®
        metrics_dict = {name: result['metrics'] for name, result in self.results.items()}
        
        evaluator = ModelEvaluator()
        comparison_df = evaluator.compare_models(metrics_dict)
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº F1 åˆ†æ•°ï¼‰
        best_name = max(metrics_dict.items(), key=lambda x: x[1]['f1_score'])[0]
        self.best_model_name = best_name
        self.best_model = self.results[best_name]['model']
        
        print(f"\nğŸ† é€‰æ‹©æœ€ä½³æ¨¡å‹: {best_name}")
        
        return comparison_df
    
    def evaluate_on_test(self, X_test, y_test):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹"""
        print("\n" + "=" * 80)
        print(f"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        print("=" * 80)
        
        # é¢„æµ‹
        y_test_pred = self.best_model.predict(X_test)
        
        try:
            y_test_pred_proba = self.best_model.predict_proba(X_test)
        except:
            y_test_pred_proba = None
        
        # è¯„ä¼°
        evaluator = ModelEvaluator(self.best_model_name)
        test_metrics = evaluator.evaluate(y_test, y_test_pred, y_test_pred_proba)
        evaluator.evaluate_per_class(y_test, y_test_pred)
        
        return test_metrics
    
    def save_best_model(self, save_path='saved_models/best_model.pkl'):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        model_info = {
            'model': self.best_model,
            'model_name': self.best_model_name,
            'metrics': self.results[self.best_model_name]['metrics']
        }
        
        joblib.dump(model_info, save_path)
        print(f"\nâœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {save_path}")
        print(f"  æ¨¡å‹: {self.best_model_name}")
        print(f"  F1åˆ†æ•°: {model_info['metrics']['f1_score']:.4f}")
    
    def get_feature_importance(self, feature_columns):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰"""
        print("\n" + "=" * 80)
        print(f"ç‰¹å¾é‡è¦æ€§åˆ†æ: {self.best_model_name}")
        print("=" * 80)
        
        try:
            if hasattr(self.best_model, 'feature_importances_'):
                # æ ‘æ¨¡å‹
                importances = self.best_model.feature_importances_
            elif hasattr(self.best_model, 'coef_'):
                # çº¿æ€§æ¨¡å‹
                importances = np.abs(self.best_model.coef_).mean(axis=0)
            else:
                print("âš ï¸  è¯¥æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
                return None
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§ DataFrame
            feature_importance_df = pd.DataFrame({
                'ç‰¹å¾': feature_columns,
                'é‡è¦æ€§': importances
            }).sort_values('é‡è¦æ€§', ascending=False)
            
            # æ˜¾ç¤º Top 20
            print("\nTop 20 é‡è¦ç‰¹å¾:")
            print("-" * 60)
            for idx, row in feature_importance_df.head(20).iterrows():
                print(f"{row['ç‰¹å¾']:<40} {row['é‡è¦æ€§']:.6f}")
            
            return feature_importance_df
            
        except Exception as e:
            print(f"âš ï¸  ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return None


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("\n" + "=" * 80)
    print("HIV é£é™©è¯„ä¼°æ¨¡å‹è®­ç»ƒ")
    print("=" * 80)
    
    # 1. ç‰¹å¾å·¥ç¨‹
    print("\næ­¥éª¤ 1: ç‰¹å¾å·¥ç¨‹")
    engineer = FeatureEngineer()
    data = engineer.process_pipeline('data/processed/hiv_data_processed.csv')
    
    # 2. åˆå§‹åŒ–è®­ç»ƒå™¨
    print("\næ­¥éª¤ 2: åˆå§‹åŒ–è®­ç»ƒå™¨")
    trainer = HIVRiskModelTrainer()
    trainer.initialize_models()
    
    # 3. è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    print("\næ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹")
    trainer.train_all_models(
        data['X_train'], data['y_train'],
        data['X_val'], data['y_val']
    )
    
    # 4. æ¯”è¾ƒæ¨¡å‹
    print("\næ­¥éª¤ 4: æ¯”è¾ƒæ¨¡å‹æ€§èƒ½")
    comparison_df = trainer.compare_models()
    
    # 5. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\næ­¥éª¤ 5: æµ‹è¯•é›†è¯„ä¼°")
    test_metrics = trainer.evaluate_on_test(data['X_test'], data['y_test'])
    
    # 6. ç‰¹å¾é‡è¦æ€§
    print("\næ­¥éª¤ 6: ç‰¹å¾é‡è¦æ€§åˆ†æ")
    feature_importance = trainer.get_feature_importance(data['feature_columns'])
    
    # 7. ä¿å­˜æ¨¡å‹
    print("\næ­¥éª¤ 7: ä¿å­˜æœ€ä½³æ¨¡å‹")
    trainer.save_best_model()
    
    print("\n" + "=" * 80)
    print("âœ“ è®­ç»ƒæµç¨‹å®Œæˆï¼")
    print("=" * 80)
    
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - saved_models/scaler.pkl (ç‰¹å¾æ ‡å‡†åŒ–å™¨)")
    print("  - saved_models/best_model.pkl (æœ€ä½³æ¨¡å‹)")
    
    return trainer, data, feature_importance


if __name__ == '__main__':
    trainer, data, feature_importance = main()
